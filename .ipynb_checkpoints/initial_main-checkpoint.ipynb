{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID, label, features = load_data(sub_sample=False, add_outlier=False)\n",
    "label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, tx = build_model_data(features, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000,), (250000, 31))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isnan(tx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isnan(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=0.328666\n",
      "Gradient Descent(1/49): loss=5609338.473922181\n",
      "Gradient Descent(2/49): loss=184376856982264.5\n",
      "Gradient Descent(3/49): loss=6.06129828214377e+21\n",
      "Gradient Descent(4/49): loss=1.992622513707655e+29\n",
      "Gradient Descent(5/49): loss=6.550650214007458e+36\n",
      "Gradient Descent(6/49): loss=2.153494599785466e+44\n",
      "Gradient Descent(7/49): loss=7.079509422421913e+51\n",
      "Gradient Descent(8/49): loss=2.3273545086741037e+59\n",
      "Gradient Descent(9/49): loss=7.65106547056853e+66\n",
      "Gradient Descent(10/49): loss=2.515250797278702e+74\n",
      "Gradient Descent(11/49): loss=8.268765438679461e+81\n",
      "Gradient Descent(12/49): loss=2.7183166765665405e+89\n",
      "Gradient Descent(13/49): loss=8.936334703043502e+96\n",
      "Gradient Descent(14/49): loss=2.9377768459885856e+104\n",
      "Gradient Descent(15/49): loss=9.657799403918058e+111\n",
      "Gradient Descent(16/49): loss=3.1749548796969472e+119\n",
      "Gradient Descent(17/49): loss=1.0437510727362903e+127\n",
      "Gradient Descent(18/49): loss=3.4312812090800973e+134\n",
      "Gradient Descent(19/49): loss=1.1280171147437266e+142\n",
      "Gradient Descent(20/49): loss=3.7083017497591527e+149\n",
      "Gradient Descent(21/49): loss=1.2190862787034128e+157\n",
      "Gradient Descent(22/49): loss=4.007687225073971e+164\n",
      "Gradient Descent(23/49): loss=1.3175078068390458e+172\n",
      "Gradient Descent(24/49): loss=4.331243242291262e+179\n",
      "Gradient Descent(25/49): loss=1.4238752838134098e+187\n",
      "Gradient Descent(26/49): loss=4.680921182302886e+194\n",
      "Gradient Descent(27/49): loss=1.5388302166640422e+202\n",
      "Gradient Descent(28/49): loss=5.058829968492127e+209\n",
      "Gradient Descent(29/49): loss=1.6630659037612002e+217\n",
      "Gradient Descent(30/49): loss=5.467248785745223e+224\n",
      "Gradient Descent(31/49): loss=1.7973316161212247e+232\n",
      "Gradient Descent(32/49): loss=5.908640826317667e+239\n",
      "Gradient Descent(33/49): loss=1.9424371162941143e+247\n",
      "Gradient Descent(34/49): loss=6.385668145457985e+254\n",
      "Gradient Descent(35/49): loss=2.0992575420775346e+262\n",
      "Gradient Descent(36/49): loss=6.901207716382786e+269\n",
      "Gradient Descent(37/49): loss=2.2687386845125978e+277\n",
      "Gradient Descent(38/49): loss=7.458368781430828e+284\n",
      "Gradient Descent(39/49): loss=2.451902691991757e+292\n",
      "Gradient Descent(40/49): loss=8.060511604044167e+299\n",
      "Gradient Descent(41/49): loss=inf\n",
      "Gradient Descent(42/49): loss=inf\n",
      "Gradient Descent(43/49): loss=inf\n",
      "Gradient Descent(44/49): loss=inf\n",
      "Gradient Descent(45/49): loss=inf\n",
      "Gradient Descent(46/49): loss=inf\n",
      "Gradient Descent(47/49): loss=inf\n",
      "Gradient Descent(48/49): loss=inf\n",
      "Gradient Descent(49/49): loss=inf\n",
      "Gradient Descent: w=[ 7.54003883e+182 -3.79349657e+182 -5.71671854e+182 -2.20054230e+182\n",
      "  7.10152316e+183  7.21502157e+183  7.10044720e+183 -1.77761699e+181\n",
      " -1.01916062e+182 -6.96051264e+182 -9.82962372e+180  3.49568492e+180\n",
      "  7.10106331e+183 -2.46958555e+182  1.13287968e+179  8.82818778e+178\n",
      " -3.05249688e+182  2.12111297e+179 -3.03124652e+179 -2.38432431e+182\n",
      "  1.24032868e+179 -1.05080200e+183 -2.36769673e+180  4.24627960e+183\n",
      "  4.40266100e+183  4.40269937e+183  7.11085976e+183  7.10096666e+183\n",
      "  7.10096805e+183 -1.43843018e+182 -6.96137106e+180]\n",
      "Gradient Descent: execution time=0.582 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Juliane/Documents/EPFL/Master semestre 4/Machine Learning/PROJET1_Ju/helpers.py:50: RuntimeWarning: overflow encountered in square\n",
      "  return 1/2*np.mean(e**2)\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.001\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(31)\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_weight, gradient_loss = least_squares_GD(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Juliane/Documents/EPFL/Master semestre 4/Machine Learning/PROJET1_Ju/helpers.py:94: RuntimeWarning: overflow encountered in multiply\n",
      "  grad = -tx.T*err\n"
     ]
    }
   ],
   "source": [
    "#Stochastic Gradient descent\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.001\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(31)\n",
    "\n",
    "\n",
    "# Start SGD\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = least_squares_SGD(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least squares: loss=0.08486139962234228\n",
      "Least squares: execution time=0.041 seconds\n",
      "Weight vector :  [-3.61501160e-05  3.57239779e-03  3.12025233e-03  2.38546990e-04\n",
      "  1.54181466e-03 -2.24707205e-04  1.19078311e-02 -1.71080338e-01\n",
      "  5.49836312e-05  1.42066688e+00  1.08108777e-01 -4.77719935e-02\n",
      " -2.26022754e-02 -1.42495013e+00  1.31359991e-04  4.97821270e-04\n",
      " -1.42718721e+00  1.77331355e-04 -4.24910792e-04 -1.84623920e-03\n",
      " -1.09056687e-04  2.50287428e-04  1.05761176e-01  1.09441063e-04\n",
      " -1.00774774e-04 -1.71567930e-04  3.49202072e-05  3.29035511e-03\n",
      "  5.91533016e-03 -1.42029063e+00  6.80329123e-01]\n"
     ]
    }
   ],
   "source": [
    "#Least squares\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_w, sgd_loss = least_squares(y, tx)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Least squares: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression: loss=0.08486139962234228\n",
      "Least squares: execution time=0.137 seconds\n",
      "Weight vector :  [-3.61501160e-05  3.57239779e-03  3.12025233e-03  2.38546990e-04\n",
      "  1.54181466e-03 -2.24707205e-04  1.19078311e-02 -1.71080338e-01\n",
      "  5.49836312e-05  1.42066688e+00  1.08108777e-01 -4.77719935e-02\n",
      " -2.26022754e-02 -1.42495013e+00  1.31359991e-04  4.97821270e-04\n",
      " -1.42718721e+00  1.77331355e-04 -4.24910792e-04 -1.84623920e-03\n",
      " -1.09056687e-04  2.50287428e-04  1.05761176e-01  1.09441063e-04\n",
      " -1.00774774e-04 -1.71567930e-04  3.49202072e-05  3.29035511e-03\n",
      "  5.91533016e-03 -1.42029063e+00  6.80329123e-01]\n"
     ]
    }
   ],
   "source": [
    "#Ridge regression\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "lambda_ = 0\n",
    "ridge_w, ridge_loss = ridge_regression(y, tx, lambda_)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Ridge regression: execution time={t:.3f} seconds\".format(t=exection_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression: loss=173286.79513998204\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: loss=nan\n",
      "Logistic regression: execution time=5.699 seconds\n",
      "Weight vector :  [-1.67344105e+04  6.18172409e+03  3.40219873e+03 -1.42407976e+03\n",
      "  3.92519432e+03  1.46629829e+03  3.97137231e+03  5.43382935e+01\n",
      "  8.75161189e+02  3.07812151e+03  1.31287208e+02 -9.18410171e+01\n",
      "  3.89696909e+03 -1.47280860e+03  1.48540608e+00  4.26716603e+00\n",
      "  1.99855738e+03  8.98694678e-01 -3.56923344e+00  8.93377961e+02\n",
      " -5.59317999e+00  4.37925395e+03  6.80853115e+01 -4.39959348e+03\n",
      " -4.51997987e+03 -4.52016701e+03  5.42821138e+03  3.89566536e+03\n",
      "  3.89825156e+03  2.55237200e+03  2.65896607e+01]\n"
     ]
    }
   ],
   "source": [
    "#Logistic regression with Gradient Descent\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.0001\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(31)\n",
    "\n",
    "logistic_w, logistic_loss = logistic_regression(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Logistic regression: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n",
    "# ISSUE : DIVISION BY ZERO IN THE SIGMOID FUNCTION (thus dot product tx and w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized logistic regression (0/49): loss=173286.79513998204\n",
      "Regularized logistic regression (1/49): loss=nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Juliane/Documents/EPFL/Master semestre 4/Machine Learning/PROJET1_Ju/helpers.py:158: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized logistic regression (2/49): loss=nan\n",
      "Regularized logistic regression (3/49): loss=nan\n",
      "Regularized logistic regression (4/49): loss=nan\n",
      "Regularized logistic regression (5/49): loss=nan\n",
      "Regularized logistic regression (6/49): loss=nan\n",
      "Regularized logistic regression (7/49): loss=nan\n",
      "Regularized logistic regression (8/49): loss=nan\n",
      "Regularized logistic regression (9/49): loss=nan\n",
      "Regularized logistic regression (10/49): loss=nan\n",
      "Regularized logistic regression (11/49): loss=nan\n",
      "Regularized logistic regression (12/49): loss=nan\n",
      "Regularized logistic regression (13/49): loss=nan\n",
      "Regularized logistic regression (14/49): loss=nan\n",
      "Regularized logistic regression (15/49): loss=nan\n",
      "Regularized logistic regression (16/49): loss=nan\n",
      "Regularized logistic regression (17/49): loss=nan\n",
      "Regularized logistic regression (18/49): loss=nan\n",
      "Regularized logistic regression (19/49): loss=nan\n",
      "Regularized logistic regression (20/49): loss=nan\n",
      "Regularized logistic regression (21/49): loss=nan\n",
      "Regularized logistic regression (22/49): loss=nan\n",
      "Regularized logistic regression (23/49): loss=nan\n",
      "Regularized logistic regression (24/49): loss=nan\n",
      "Regularized logistic regression (25/49): loss=nan\n",
      "Regularized logistic regression (26/49): loss=nan\n",
      "Regularized logistic regression (27/49): loss=nan\n",
      "Regularized logistic regression (28/49): loss=nan\n",
      "Regularized logistic regression (29/49): loss=nan\n",
      "Regularized logistic regression (30/49): loss=nan\n",
      "Regularized logistic regression (31/49): loss=nan\n",
      "Regularized logistic regression (32/49): loss=nan\n",
      "Regularized logistic regression (33/49): loss=nan\n",
      "Regularized logistic regression (34/49): loss=nan\n",
      "Regularized logistic regression (35/49): loss=nan\n",
      "Regularized logistic regression (36/49): loss=nan\n",
      "Regularized logistic regression (37/49): loss=nan\n",
      "Regularized logistic regression (38/49): loss=nan\n",
      "Regularized logistic regression (39/49): loss=nan\n",
      "Regularized logistic regression (40/49): loss=nan\n",
      "Regularized logistic regression (41/49): loss=nan\n",
      "Regularized logistic regression (42/49): loss=nan\n",
      "Regularized logistic regression (43/49): loss=nan\n",
      "Regularized logistic regression (44/49): loss=nan\n",
      "Regularized logistic regression (45/49): loss=nan\n",
      "Regularized logistic regression (46/49): loss=nan\n",
      "Regularized logistic regression (47/49): loss=nan\n",
      "Regularized logistic regression (48/49): loss=nan\n",
      "Regularized logistic regression (49/49): loss=nan\n",
      "Regularized logistic regression: w=[-1.67344105e+04  6.18172409e+03  3.40219873e+03 -1.42407976e+03\n",
      "  3.92519432e+03  1.46629829e+03  3.97137231e+03  5.43382935e+01\n",
      "  8.75161189e+02  3.07812151e+03  1.31287208e+02 -9.18410171e+01\n",
      "  3.89696909e+03 -1.47280860e+03  1.48540608e+00  4.26716603e+00\n",
      "  1.99855738e+03  8.98694678e-01 -3.56923344e+00  8.93377961e+02\n",
      " -5.59317999e+00  4.37925395e+03  6.80853115e+01 -4.39959348e+03\n",
      " -4.51997987e+03 -4.52016701e+03  5.42821138e+03  3.89566536e+03\n",
      "  3.89825156e+03  2.55237200e+03  2.65896607e+01]\n",
      "Regularized logistic regression: execution time=6.126 seconds\n"
     ]
    }
   ],
   "source": [
    "#Regularized logistic regression with Gradient Descent\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.0001\n",
    "\n",
    "lambda_ = 0\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(31)\n",
    "\n",
    "reg_logistic_w, reg_logistic_loss = reg_logistic_regression(y, tx, lambda_, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Regularized logistic regression: execution time={t:.3f} seconds\".format(t=exection_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
