{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from proj1_helpers import *\n",
    "from implementations import * \n",
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = os.path.dirname(os.getcwd()) + '/data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH) # labels/predictions, id of each sample, tX - 30 features of each sample (float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_explore, tX_explore, ids_explore = load_csv_data(DATA_TRAIN_PATH,sub_sample = True) # subsample to do data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_feature = np.genfromtxt(DATA_TRAIN_PATH, delimiter=\",\", dtype=str, max_rows=1)[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(y)\n",
    "num_features0 = tX.shape[1]\n",
    "\n",
    "print(\"Number of samples in train data set: {}\".format(num_samples))\n",
    "print(\"Initial number of features in train data set : {}\".format(num_features0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of Nan values in train data set : {}\".format(np.argwhere(np.isnan(tX)).shape[0]))\n",
    "print(\"Number of Nan values in train data set : {}\".format(np.count_nonzero(np.where(tX == np.nan, 1,0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tX.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_ = []\n",
    "for i in range(num_features0):\n",
    "    range_.append(len(np.unique(tX_explore[:,i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the proportion of the 2 class\n",
    "labels = ['Not Boson','Boson']\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 4))\n",
    "axes[0].set_title('Portion of Bosons in the data set ')\n",
    "axes[0].bar([-1,1],[np.sum(y==-1),np.sum(y==1)], color='salmon', edgecolor='black', linewidth=1, log = False);\n",
    "axes[0].set_ylabel('Number of samples')\n",
    "axes[0].set_xticks([-1,1])\n",
    "axes[0].set_xticklabels(labels)\n",
    "\n",
    "axes[1].set_title('Distribution of the size of the range of values for each feature vector')\n",
    "axes[1].bar(np.arange(num_features0)+1,range_, color='salmon', edgecolor='black', linewidth=1, log = True);\n",
    "axes[1].set_xlabel('Number of each feature')\n",
    "axes[1].set_ylabel('log(number of distinct feature values)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the proportion is not equal, we have to make sure the optimisation procedure takes this into account (proportion of classes in each set, class error instead of classification error)\n",
    "We see that one feature among all features exhibit a much more restricted range of values, probably a category feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_explore[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(tX_explore[:,22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1 = ['Not Boson','Boson']\n",
    "labels0 = ['0','1','2','3']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].bar(np.arange(4),[np.sum(tX_explore==0.),np.sum(tX_explore==1.),np.sum(tX_explore==2.),np.sum(tX_explore==3.)],width=0.5, color='salmon', align='center',edgecolor='black')\n",
    "axes[0].set_title('Distribution of feature 22 value across data set')\n",
    "axes[0].set_ylabel('Number of samples')\n",
    "axes[0].set_xticks(np.arange(4))\n",
    "axes[0].set_xticklabels(labels0)\n",
    "axes[1].bar(np.arange(1,3)-0.4, [np.sum(y_explore[tX_explore[:,22]==0.] ==-1)/np.sum(y_explore == -1),np.sum(y_explore[tX_explore[:,22]==0.] ==1)/np.sum(y_explore == 1)], width=0.2, color='salmon', align='center',edgecolor='black',label = '0')\n",
    "axes[1].bar(np.arange(1,3)-0.2, [np.sum(y_explore[tX_explore[:,22]==1.] ==-1)/np.sum(y_explore == -1),np.sum(y_explore[tX_explore[:,22]==2.] ==1)/np.sum(y_explore == 1)], width=0.2, color='tomato', align='center',edgecolor='black',label = '1')\n",
    "axes[1].bar(np.arange(1,3), [np.sum(y_explore[tX_explore[:,22]==2.] ==-1)/np.sum(y_explore == -1),np.sum(y_explore[tX_explore[:,22]==3.] ==1)/np.sum(y_explore == 1)], width=0.2, color='coral', align='center',edgecolor='black',label = '2')\n",
    "axes[1].bar(np.arange(1,3)+0.2, [np.sum(y_explore[tX_explore[:,22]==3.] ==-1)/np.sum(y_explore == -1),np.sum(y_explore[tX_explore[:,22]==4.] ==1)/np.sum(y_explore == 1)], width=0.2, color='darksalmon', align='center',edgecolor='black',label = '3')\n",
    "axes[1].set_title('Proportion of each class with different feature 22 value')\n",
    "axes[1].set_ylabel('Number of samples')\n",
    "axes[1].set_xticks([0.9,1.9])\n",
    "axes[1].set_xticklabels(labels1)\n",
    "axes[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Dealing with undefined values* ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset based on the value of PRI_jet_num and \n",
    "# removing undefined features for the corresponding subsets\n",
    "ss0_tX, ss0_y, ss1_tX, ss1_y, ss2_tX, ss2_y, ss3_tX, ss3_y = split_subsets(tX, y,labels_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ss0_tX.shape)\n",
    "print(ss1_tX.shape)\n",
    "print(ss2_tX.shape)\n",
    "print(ss3_tX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of remaining undefined values in subset 0 : {}\".format(np.count_nonzero(ss0_tX == -999.0)))\n",
    "undefined_indices = np.argwhere(ss0_tX == -999.0)\n",
    "\n",
    "unique_elements, counts_elements = np.unique(undefined_indices[:,1], return_counts=True)\n",
    "print(\"Feature repartition of these values : {}\".format(np.asarray((unique_elements, counts_elements))))\n",
    "\n",
    "print(\"Number of remaining undefined values in subset 1 : {}\".format(np.count_nonzero(ss1_tX == -999.0)))\n",
    "undefined_indices = np.argwhere(ss1_tX == -999.0)\n",
    "\n",
    "unique_elements, counts_elements = np.unique(undefined_indices[:,1], return_counts=True)\n",
    "print(\"Feature repartition of these values : {}\".format(np.asarray((unique_elements, counts_elements))))\n",
    "\n",
    "print(\"Number of remaining undefined values in subset 2 : {}\".format(np.count_nonzero(ss2_tX == -999.0)))\n",
    "undefined_indices = np.argwhere(ss2_tX == -999.0)\n",
    "\n",
    "unique_elements, counts_elements = np.unique(undefined_indices[:,1], return_counts=True)\n",
    "print(\"Feature repartition of these values : {}\".format(np.asarray((unique_elements, counts_elements))))\n",
    "\n",
    "print(\"Number of remaining undefined values in subset 3 : {}\".format(np.count_nonzero(ss3_tX == -999.0)))\n",
    "undefined_indices = np.argwhere(ss3_tX == -999.0)\n",
    "\n",
    "unique_elements, counts_elements = np.unique(undefined_indices[:,1], return_counts=True)\n",
    "print(\"Feature repartition of these values : {}\".format(np.asarray((unique_elements, counts_elements))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices of column containing remaining -999 \n",
    "print('Columns where -999 remain in subset 0: {}'.format(np.unique(np.argwhere(np.where(ss0_tX == -999.0,1,0))[:,1])))\n",
    "print('Columns where -999 remain in subset 1: {}'.format(np.unique(np.argwhere(np.where(ss1_tX == -999.0,1,0))[:,1])))\n",
    "print('Columns where -999 remain in subset 2: {}'.format(np.unique(np.argwhere(np.where(ss2_tX == -999.0,1,0))[:,1])))\n",
    "print('Columns where -999 remain in subset 3: {}'.format(np.unique(np.argwhere(np.where(ss3_tX == -999.0,1,0))[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus all remaining undefined values belong to the first feature **DER_mass_MMC**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING DIFFERENT SUBSETS OF FEATURES TO TEST\n",
    "\n",
    "# Subset feature 1 : only primitive features w\n",
    "ss0_tX1 = ss0_tX[:,13:]\n",
    "ss1_tX1 = ss1_tX[:,13:]\n",
    "ss2_tX1 = ss2_tX[:,13:]\n",
    "ss3_tX1 = ss3_tX[:,13:]\n",
    "# Subset feature 2 : primitive and derivated features with removal of DER_mass_MMC\n",
    "ss0_tX2 = ss0_tX[:,1:]\n",
    "ss1_tX2 = ss1_tX[:,1:]\n",
    "ss2_tX2 = ss2_tX[:,1:]\n",
    "ss3_tX2 = ss3_tX[:,1:]\n",
    "# Subset feature 3 : primitive and derivated features with replacemnet of DET_mass_MMC by median of defined values\n",
    "ss0_tX3 = replace_undef_feat(ss0_tX,method = 'median')\n",
    "ss1_tX3 = replace_undef_feat(ss1_tX,method = 'median')\n",
    "ss2_tX3 = replace_undef_feat(ss2_tX,method = 'median')\n",
    "ss3_tX3 = replace_undef_feat(ss3_tX,method = 'median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices of column containing remaining -999 \n",
    "print('Columns where -999 remain in subset 0: {}'.format(np.unique(np.argwhere(np.where(ss0_tX1 == -999.0,1,0))[:,1])))\n",
    "print('Columns where -999 remain in subset 1: {}'.format(np.unique(np.argwhere(np.where(ss1_tX1 == -999.0,1,0))[:,1])))\n",
    "print('Columns where -999 remain in subset 2: {}'.format(np.unique(np.argwhere(np.where(ss2_tX1 == -999.0,1,0))[:,1])))\n",
    "print('Columns where -999 remain in subset 3: {}'.format(np.unique(np.argwhere(np.where(ss3_tX1 == -999.0,1,0))[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Dealing with outliers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plotting sccatter of the features\n",
    "\n",
    "for i in range(0,np.size(ss0_tX,1)-1,2):\n",
    "    fig = scatter_visualization(ss0_y, ss0_tX[:,i], ss0_tX[:,i+1],i)\n",
    "    fig.set_size_inches(25.0,4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plotting histograms of the features\n",
    "for i in range(0,np.size(ss0_tX,1)-1,2):\n",
    "    histo_visualization(ss0_tX[:,i], tX_explore[:,i+1],i, 3)\n",
    "\n",
    "for i in range(0,np.size(ss1_tX,1)-1,2):\n",
    "    histo_visualization(ss1_tX[:,i], tX_explore[:,i+1],i, 3)\n",
    "    \n",
    "for i in range(0,np.size(ss2_tX,1)-1,2):\n",
    "    histo_visualization(ss2_tX[:,i], tX_explore[:,i+1],i, 3)\n",
    "    \n",
    "for i in range(0,np.size(ss3_tX,1)-1,2):\n",
    "    histo_visualization(ss3_tX[:,i], tX_explore[:,i+1],i, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats #je sais on a pas le droit, mais c'est pour comparé ! \n",
    "\n",
    "z = np.abs(stats.zscore(tX_explore))\n",
    "threshold = 3\n",
    "tX_explore_o = tX_explore[(z < 3).all(axis=1)]\n",
    "print (tX_explore.shape, tX_explore_o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Même chose qu'avec z scor mais 3 std\n",
    "deviation_feature = np.std(tX_explore,axis = 0)\n",
    "mean_feature = np.mean(tX_explore,axis = 0)\n",
    "index = []\n",
    "for i in range(np.size(tX_explore,1)):\n",
    "    dev_idx = deviation_feature[i]\n",
    "    mean_idx = mean_feature[i]\n",
    "    threshold = (3*dev_idx) + mean_idx\n",
    "    for j in range(np.size(tX_explore,0)):\n",
    "        if abs(tX_explore[j,i]) > threshold:\n",
    "            index.append(j)\n",
    "\n",
    "tX_explore_outliers = np.delete(tX_explore, index, 0)\n",
    "print (tX_explore.shape, tX_explore_outliers.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Feature selection* ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a heat map of the correlations between features\n",
    "# SUBSET 0\n",
    "labels_0 = [\"DER_mass_MMC\",\"DER_mass_transverse_met_lep\",\"DER_mass_vis\",\"DER_pt_h\",\"DER_deltaeta_jet_jet\", \\\n",
    "          \"DER_pt_tot\",\"DER_sum_pt\",\"DER_pt_ratio_lep_tau\", \\\n",
    "          \"DER_met_phi_centrality\",\"PRI_tau_pt\",\"PRI_tau_eta\",\"PRI_tau_phi\",\"PRI_lep_pt\", \\\n",
    "          \"PRI_lep_eta\",\"PRI_lep_phi\",\"PRI_met\",\"PRI_met_phi\",\"PRI_met_sumet\", \"Output\"]\n",
    "\n",
    "ranked_index_ss0, ranked_features_ss0 = plot_correlation_matrix(ss0_tX, ss0_y, labels_0, \"CorrelationMatrix_ss0.png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBSET 1\n",
    "labels_1 = [\"DER_mass_MMC\",\"DER_mass_transverse_met_lep\",\"DER_mass_vis\",\"DER_pt_h\",\"DER_deltaeta_jet_jet\", \\\n",
    "          \"DER_pt_tot\",\"DER_sum_pt\",\"DER_pt_ratio_lep_tau\", \\\n",
    "          \"DER_met_phi_centrality\",\"PRI_tau_pt\",\"PRI_tau_eta\",\"PRI_tau_phi\",\"PRI_lep_pt\", \\\n",
    "          \"PRI_lep_eta\",\"PRI_lep_phi\",\"PRI_met\",\"PRI_met_phi\",\"PRI_met_sumet\", \"PRI_jet_leading_pt\", \\\n",
    "          \"PRI_jet_leading_eta\", \"PRI_jet_leading_phi\", \"PRI_jet_all_pt\", \"Output\"]\n",
    "\n",
    "ranked_index_ss1, ranked_features_ss1 = plot_correlation_matrix(ss1_tX, ss1_y, labels_1, \"CorrelationMatrix_ss1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBSET 2\n",
    "labels_2_3 = [\"DER_mass_MMC\", \"DER_mass_transverse_met_lep\", \"DER_mass_vis\",\"DER_pt_h\",\\\n",
    "              \"DER_deltaeta_jet_jet\",\"DER_mass_jet_jet\",\"DER_prodeta_jet_jet\",\"DER_deltar_tau_lep\",\\\n",
    "              \"DER_pt_tot\",\"DER_sum_pt\",\"DER_pt_ratio_lep_tau\",\"DER_met_phi_centrality\",\"DER_lep_eta_centrality\",\\\n",
    "              \"PRI_tau_pt\",\"PRI_tau_eta\",\"PRI_tau_phi\",\"PRI_lep_pt\",\"PRI_lep_eta\",\"PRI_lep_phi\",\\\n",
    "              \"PRI_met\",\"PRI_met_phi\",\"PRI_met_sumet\",\"PRI_jet_leading_pt\",\"PRI_jet_leading_eta\", \\\n",
    "              \"PRI_jet_leading_phi\",\"PRI_jet_subleading_pt\",\"PRI_jet_subleading_eta\",\"PRI_jet_subleading_phi\",\\\n",
    "              \"PRI_jet_all_pt\", \"Output\"]\n",
    "\n",
    "ranked_index_ss2, ranked_features_ss2 = plot_correlation_matrix(ss2_tX, ss2_y, labels_2_3, \"CorrelationMatrix_ss2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBSET 3\n",
    "ranked_index_ss3, ranked_features_ss3 = plot_correlation_matrix(ss3_tX, ss3_y, labels_2_3, \"CorrelationMatrix_ss3.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Feature augmentation* ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation + standardization se faire automatiquement dans la cross val, suffit de specifier le degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection and Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ls, loss_ls = least_squares(y_explore, tX_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of correct classification on train set : {}'.format(loss_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Square Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(10)\n",
    "max_iters = 500\n",
    "gamma = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ls_GD, loss_ls_GD = least_squares_GD(y, tX_sd[:,:10], initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final class error on train data : {}'.format(loss_ls_GD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Square Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(3)\n",
    "max_iters = 500\n",
    "gamma = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ls_SGD, loss_ls_SGC = least_squares_SGD(y, tX[:,:3], initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import * \n",
    "lambdas = [0.1,0.2,0.3]\n",
    "degree = 1\n",
    "seed = 18\n",
    "initial_w = np.zeros(ss2_tX2.shape[1]+1)\n",
    "max_iters = 1000\n",
    "gamma = np.linspace(0.001,0.2,20)\n",
    "batch_size = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tr, loss_te, w = cross_validation_demo(ss2_y, ss2_tX2, degree, seed, k_fold = 5, class_distribution = True, error = 'class',method = 'lsGD',hyperparams = [initial_w,max_iters,gamma])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].boxplot(loss_tr)\n",
    "axes[0].set_title('Train : Errors across folds for different lambda value')\n",
    "axes[0].set_ylabel('Error')\n",
    "axes[1].boxplot(loss_te)\n",
    "axes[1].set_title('Test : Errors across folds for different lambda value')\n",
    "axes[1].set_ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(3)\n",
    "max_iters = 500\n",
    "gamma = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression(y, tX[:,:3], initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = os.path.dirname(os.getcwd()) + '/data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss0_tX_test, index0, ss1_tX_test, index1, ss2_tX_test, index2, ss3_tX_test, index3= split_subsets_test(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A CHANGER POUR CHAQUE SUBMISSION AVEC BEST WEIGHTS !!!\n",
    "weights0 = np.ones(ss0_tX_test.shape[1])\n",
    "weights1 = np.ones(ss1_tX_test.shape[1])\n",
    "weights2 = np.ones(ss2_tX_test.shape[1])\n",
    "weights3 = np.ones(ss3_tX_test.shape[1])\n",
    "\n",
    "# NE PLUS RIEN CHANGER A PARTIR D'ICI\n",
    "# Subset 0 \n",
    "y_pred0 = predict_labels(weights0, ss0_tX_test)\n",
    "#Subset 1\n",
    "y_pred1 = predict_labels(weights1, ss1_tX_test)\n",
    "#Subset 2\n",
    "y_pred2 = predict_labels(weights2, ss2_tX_test)\n",
    "#Subset 3\n",
    "y_pred3 = predict_labels(weights3, ss3_tX_test)\n",
    "\n",
    "#Stack all prediction from 4 subgroups to get y_pred in corredt order \n",
    "y_pred = np.ones(len(ids_test))\n",
    "y_pred[index0] = y_pred0\n",
    "y_pred[index1] = y_pred1\n",
    "y_pred[index2] = y_pred2\n",
    "y_pred[index3] = y_pred3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = os.path.dirname(os.getcwd()) + '/data/' + str(datetime.now())# TODO: fill in desired name of output file for submission\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
